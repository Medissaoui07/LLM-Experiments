{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPvfwg20QpB8f1qFR1u77/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Medissaoui07/LLM-Experiments/blob/main/GPT_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S_fdV-NUymb",
        "outputId": "27be0dc5-b67f-4aff-846c-d9ad3e790231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZT14SYfDWoZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmJzL_ADCLz-",
        "outputId": "6da2657b-1840-4f61-8314-ecb5d1cde989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLWCn64EhXa",
        "outputId": "fdc3637b-8fff-4bd7-f833-90e39cb732bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-24 12:48:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.5’\n",
            "\n",
            "input.txt.5         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-01-24 12:48:30 (138 MB/s) - ‘input.txt.5’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "lines=text[:500]\n",
        "print(lines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqqS58a8F0xm",
        "outputId": "394707be-fc5f-4fe7-e777-bb793e2eb569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTBbfwNoJqAP",
        "outputId": "19c679a4-0a55-4c6d-ed7f-aaa35c099b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 80% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "0Fmr4Dw5Jrme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hg5M9v3OG5Q",
        "outputId": "fd8b8983-253a-4859-ff19-39df61c06a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n",
            "tensor([18, 47, 56,  ..., 43, 56, 43])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "  def forward(self,x) :\n",
        "    B,T,C=x.shape\n",
        "    k=self.key(x)\n",
        "    q=self.query(x)\n",
        "    weights=q @ k.transpose(-2,-1)*C**-0.5\n",
        "    weights=weights.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "    weights=weights.softmax(dim=-1)\n",
        "    weights=self.dropout(weights)\n",
        "    v=self.value(x)\n",
        "    out=weights @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "qQpFHnglWkqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj=nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "    out=self.proj(out)\n",
        "    out = self.dropout(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "cZOj95aTf886",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.ffn=nn.Sequential(\n",
        "        nn.Linear(n_embd,4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd,n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "      return self.ffn(x)"
      ],
      "metadata": {
        "id": "sXWNKlMaPmAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneBlock(nn.Module):\n",
        "  def __init__(self,n_embd,n_head):\n",
        "    super().__init__()\n",
        "    head_size=n_embd//n_head\n",
        "    self.sa=MultiHeadAttention(n_head,head_size)\n",
        "    self.ff=FeedForward(n_embd)\n",
        "    self.ln1=nn.LayerNorm(n_embd)\n",
        "    self.ln2=nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=x+self.sa(self.ln1(x))\n",
        "      x=x+self.ff(self.ln2(x))\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "M_fNfuNGTvOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size,n_embd,n_head,n_layer):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(vocab_size,n_embd)\n",
        "    self.pos_embedding=nn.Embedding(block_size,n_embd )\n",
        "    self.blocks=nn.Sequential(*[OneBlock(n_embd,n_head) for _ in range(n_layer)])\n",
        "    self.ln=nn.LayerNorm(n_embd)\n",
        "    self.head=nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tokens = self.embedding(idx)\n",
        "    pos = self.pos_embedding(torch.arange(T, device=device))\n",
        "    x=tokens+pos\n",
        "    x=self.blocks(x)\n",
        "    x=self.ln(x)\n",
        "    logits=self.head(x)\n",
        "    if targets is None:\n",
        "            loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = f.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = f.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "9HS1Pm8phl33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement some utils functions\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "EJI76eCjK1-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LanguageModel(vocab_size,n_embd,n_head,n_layer)\n",
        "model.to(device)\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "GygwOgsdD1jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0 or iter == max_iters-1 :\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  x,y=get_batch('train')\n",
        "  logits,loss=model(x,y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "vWliDaI4STct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d307ed8-f87d-4f49-c0bb-ac30777ad1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2905, val loss 4.2962\n",
            "step 100: train loss 2.7143, val loss 2.7412\n",
            "step 200: train loss 2.5671, val loss 2.5885\n",
            "step 300: train loss 2.4945, val loss 2.4976\n",
            "step 400: train loss 2.4450, val loss 2.4484\n",
            "step 500: train loss 2.4028, val loss 2.4042\n",
            "step 600: train loss 2.3559, val loss 2.3708\n",
            "step 700: train loss 2.3198, val loss 2.3320\n",
            "step 800: train loss 2.2988, val loss 2.3135\n",
            "step 900: train loss 2.2585, val loss 2.2879\n",
            "step 1000: train loss 2.2389, val loss 2.2574\n",
            "step 1100: train loss 2.2093, val loss 2.2323\n",
            "step 1200: train loss 2.1940, val loss 2.2242\n",
            "step 1300: train loss 2.1730, val loss 2.1982\n",
            "step 1400: train loss 2.1532, val loss 2.1902\n",
            "step 1500: train loss 2.1355, val loss 2.1741\n",
            "step 1600: train loss 2.1157, val loss 2.1589\n",
            "step 1700: train loss 2.0889, val loss 2.1509\n",
            "step 1800: train loss 2.0803, val loss 2.1387\n",
            "step 1900: train loss 2.0717, val loss 2.1330\n",
            "step 2000: train loss 2.0688, val loss 2.1278\n",
            "step 2100: train loss 2.0541, val loss 2.1163\n",
            "step 2200: train loss 2.0383, val loss 2.0972\n",
            "step 2300: train loss 2.0305, val loss 2.0999\n",
            "step 2400: train loss 2.0236, val loss 2.0906\n",
            "step 2500: train loss 2.0249, val loss 2.0805\n",
            "step 2600: train loss 2.0056, val loss 2.0753\n",
            "step 2700: train loss 1.9999, val loss 2.0706\n",
            "step 2800: train loss 1.9915, val loss 2.0627\n",
            "step 2900: train loss 1.9818, val loss 2.0745\n",
            "step 3000: train loss 1.9722, val loss 2.0696\n",
            "step 3100: train loss 1.9796, val loss 2.0604\n",
            "step 3200: train loss 1.9621, val loss 2.0376\n",
            "step 3300: train loss 1.9535, val loss 2.0373\n",
            "step 3400: train loss 1.9499, val loss 2.0410\n",
            "step 3500: train loss 1.9406, val loss 2.0426\n",
            "step 3600: train loss 1.9331, val loss 2.0200\n",
            "step 3700: train loss 1.9286, val loss 2.0233\n",
            "step 3800: train loss 1.9359, val loss 2.0199\n",
            "step 3900: train loss 1.9091, val loss 2.0232\n",
            "step 4000: train loss 1.9140, val loss 2.0134\n",
            "step 4100: train loss 1.9054, val loss 2.0022\n",
            "step 4200: train loss 1.9079, val loss 2.0146\n",
            "step 4300: train loss 1.9083, val loss 2.0055\n",
            "step 4400: train loss 1.9223, val loss 2.0123\n",
            "step 4500: train loss 1.8919, val loss 2.0008\n",
            "step 4600: train loss 1.8918, val loss 1.9861\n",
            "step 4700: train loss 1.8795, val loss 1.9789\n",
            "step 4800: train loss 1.8828, val loss 1.9951\n",
            "step 4900: train loss 1.8740, val loss 1.9824\n",
            "step 4999: train loss 1.8680, val loss 1.9898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EcIR1bkhVtQ",
        "outputId": "133c84c4-4763-4298-9167-40f7dc37996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CEMIO:\n",
            "Let donch greve, I seak.\n",
            "\n",
            "PORCAP:\n",
            "Yout like:\n",
            "Antersal why, dougar, bleace the love you to grend\n",
            "Dord migh in in fick save fol fornater,\n",
            "To swe your an Joh Cryow calonoble so your have,\n",
            "Flor, my kir. Good\n",
            "Of I lovage the sea her mora,\n",
            "Bree,\n",
            "Aefor son turbrow the she come thou kinspenct Reverx?\n",
            "\n",
            "RAJULIT:\n",
            "Is him.\n",
            "He eread:\n",
            "And hum lost in should prackn'nst.\n",
            "Sendeas,\n",
            "Ford, gety usompincen hous hall me Gurs of wed,\n",
            "Was a brike that chicfent alovk;\n",
            "I I greas King bor in saigh! co!\n",
            "I thy acnnice lead her seen: hear it you combrick!\n",
            "\n",
            "WABCUS:\n",
            "As fach:\n",
            "No cat thy jon of there reperremorcen,\n",
            "\n",
            "Sin vomatrice:\n",
            "That thou are will a all theous daw-awar: roven perinsack,\n",
            "And nin the spup to it shal that ahim\n",
            "Hersasier tham in sso tra my hinds\n",
            "Hit out wrordsing of my stinameds\n",
            "We wirsen dranesece of you bel- ear is the savet amore,\n",
            "Arn Rathers coudand;is indo it have speas,\n",
            "The sair Aft was for mene thuse fore.\n",
            "\n",
            "SENEN:\n",
            "Wlecounson yaus oe, ou, way, me's a sed;\n",
            "The kingen frium: this in preat your offor is to awart in the smow,\n",
            "\n",
            "And this fall Pratues, ir pautir,\n",
            "Honlawe denry law, with I I troys\n",
            "Whysir, hy let and core exhondsmand so.\n",
            "\n",
            "PRINIUTHBip:\n",
            "What and what any Iil is to tome, thoo connimurer dequo,\n",
            "And when thus pelwood tar; where thes.\n",
            "\n",
            "MENIUS:\n",
            "Iges, are my stent ofn the flenn the cels,\n",
            "Any ereflal in the lo? I is off\n",
            "Sfor you?\n",
            "\n",
            "LARF YORD:\n",
            "O, of thor ruct beskser no the cersid\n",
            "And fat the love he, maym word.\n",
            "' princal, are the lights trants, nove man may fead till sto you hort we,\n",
            "In dor fear, so now.\n",
            "My I unmigue ye hise bridelint\n",
            "fold'd an 't seent too combrowne,\n",
            "\n",
            "Both Trangers:\n",
            "Make:ink, your murtone ear not!\n",
            "Stellive wel his dood.\n",
            "- the Ould swer? O,\n",
            "What is my bout that me are wort was thou deet weet of make by not doo,\n",
            "Fromedon with aread his chumphes than to dgain, he choumponst\n",
            "The brout,--\n",
            "\n",
            "DUKE CINEN:\n",
            "So woor this, from such what callose desed so a GodYer Mort;\n",
            "At thye sen, alay!\n",
            "Say, she: you she, sirs when's jore, chalm, full this courser ful tuch her me ant,\n",
            "\n"
          ]
        }
      ]
    }
  ]
}